extract_stats_group <- function(file_parquet, mem_perf, ...) {
  arrow::open_dataset(file_parquet) |>
    # here we must average across different trials
    summarise(
      mean_fisher_z = mean(fisher_z, na.rm = TRUE),
      .by = c(subj_id, region_id, window_id)
    ) |>
    left_join(mem_perf, by = "subj_id", relationship = "many-to-many") |>
    collect() |>
    summarise(
      cor.test(mean_fisher_z, dprime, ...) |>
        broom::tidy(),
      .by = c(region_id, window_id, mem_type)
    )
}

# permutate subject id to get surrogate null distribution
permutate_behav <- function(data, cols_id) {
  data_ids <- unique(data[cols_id])
  data_ids_perm <- data_ids[sample.int(nrow(data_ids)), ]
  suff_tmp <- "_perm"
  names(data_ids_perm) <- paste0(cols_id, suff_tmp)
  bind_cols(data_ids, data_ids_perm) |>
    left_join(data, by = cols_id) |>
    select(-all_of(cols_id)) |>
    rename_with(
      ~ str_remove(.x, suff_tmp),
      ends_with(suff_tmp)
    )
}

extract_cluster_stats <- function(stats, col_p_value, col_t_stat,
                                  alternative = c("greater", "less"),
                                  alpha = 0.05) {
  alternative <- match.arg(alternative)
  stats |>
    mutate(
      is_sig = map2_dbl(
        {{ col_p_value }}, {{ col_t_stat }},
        convert_p2_to_p1,
        alternative = .env$alternative
      ) < alpha
    ) |>
    # order is essential for cluster detection
    arrange(window_id) |>
    summarise(
      find_largest_cluster({{ col_t_stat }}, is_sig),
      # grouping variables generated by targets' batches start with "tar"
      .by = c(region_id, mem_type, starts_with("tar"))
    )
}

find_largest_cluster <- function(statistic, is_sig) {
  clusters <- as_tibble(find_cluster(is_sig))
  if (nrow(clusters) == 0) {
    return(tibble(start = NA, end = NA, sum_t = 0))
  }
  clusters |>
    mutate(
      sum_t = map2_dbl(
        start, end,
        \(start, end) {
          sum(statistic[start:end])
        }
      )
    ) |>
    slice_max(sum_t)
}

find_cluster <- function(x, values_keep = 1) {
  # https://stackoverflow.com/a/43875717/5996475
  rle_x <- rle(x)
  end <- cumsum(rle_x$lengths)
  start <- c(1, lag(end)[-1] + 1)
  list(
    start = start[rle_x$values == values_keep],
    end = end[rle_x$values == values_keep]
  )
}

# https://www.graphpad.com/guides/prism/latest/statistics/one-tail_vs__two-tail_p_values.htm
convert_p2_to_p1 <- function(p, statistic,
                             alternative = c("greater", "less")) {
  alternative <- match.arg(alternative)
  true_dir <- xor(statistic > 0, alternative == "less")
  if (true_dir) {
    p / 2
  } else {
    1 - p / 2
  }
}
